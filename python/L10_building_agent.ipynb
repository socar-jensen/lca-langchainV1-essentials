{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# L10: Building a Single Agent from Scratch\n",
    "# L10: ì‹±ê¸€ ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° êµ¬ì¶•í•˜ê¸°\n",
    "\n",
    "ì´ ë ˆìŠ¨ì—ì„œëŠ” L1~L9ì—ì„œ ë°°ìš´ ê°œë…ë“¤ì„ ì¢…í•©í•˜ì—¬ **í•˜ë‚˜ì˜ ì™„ì„±ëœ ì—ì´ì „íŠ¸**ë¥¼ ë‹¨ê³„ë³„ë¡œ ë§Œë“¤ì–´ê°‘ë‹ˆë‹¤.\n",
    "\n",
    "í”„ë ˆì„ì›Œí¬ ì„¤ëª…ë³´ë‹¤ **ì‹¤ì œ êµ¬ì¶• ê³¼ì •**ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ìš°ë¦¬ê°€ ë§Œë“¤ ì—ì´ì „íŠ¸: ì—°êµ¬ ë„ìš°ë¯¸ (Research Assistant)\n",
    "\n",
    "**ê¸°ëŠ¥:**\n",
    "- ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ìˆ˜ì§‘\n",
    "- ë©”ëª¨ ì €ì¥ ë° ì¡°íšŒ\n",
    "- ëŒ€í™” ê¸°ì–µ (ì„¸ì…˜ ìœ ì§€)\n",
    "- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\n",
    "- êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "- ì‚¬ìš©ì ê¶Œí•œì— ë”°ë¥¸ ë™ì  ë™ì‘\n",
    "- ë¯¼ê°í•œ ì‘ì—… ì „ ì‚¬ìš©ì ìŠ¹ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "## ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env, doublecheck_pkgs\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print results\n",
    "doublecheck_env(\"example.env\")  # check environmental variables\n",
    "doublecheck_pkgs(pyproject_path=\"pyproject.toml\", verbose=True)   # check packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: ê¸°ë³¸ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "\n",
    "ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ ì—ì´ì „íŠ¸ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1-basic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# ê°€ì¥ ê¸°ë³¸ì ì¸ ì—ì´ì „íŠ¸: ëª¨ë¸ + ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "agent_v1 = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    system_prompt=\"You are a research assistant. Help users find and organize information.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "result = agent_v1.invoke({\"messages\": \"AI ì—ì´ì „íŠ¸ì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œê°€ ë­ì•¼?\"})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-problem",
   "metadata": {},
   "source": [
    "**ë¬¸ì œì :** ì—ì´ì „íŠ¸ê°€ ìì²´ ì§€ì‹ì—ë§Œ ì˜ì¡´í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì •ë³´ë‚˜ ì™¸ë¶€ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: ë„êµ¬ ì¶”ê°€\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë„êµ¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "# ë©”ëª¨ ì €ì¥ì†Œ (ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ êµ¬í˜„)\n",
    "notes_db = {}\n",
    "\n",
    "@tool\n",
    "def save_note(title: str, content: str) -> str:\n",
    "    \"\"\"Save a research note with a title and content.\"\"\"\n",
    "    notes_db[title] = {\n",
    "        \"content\": content,\n",
    "        \"created_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    return f\"Note '{title}' saved successfully.\"\n",
    "\n",
    "@tool\n",
    "def get_note(title: str) -> str:\n",
    "    \"\"\"Retrieve a saved note by its title.\"\"\"\n",
    "    if title in notes_db:\n",
    "        note = notes_db[title]\n",
    "        return f\"Title: {title}\\nContent: {note['content']}\\nCreated: {note['created_at']}\"\n",
    "    return f\"Note '{title}' not found.\"\n",
    "\n",
    "@tool\n",
    "def list_notes() -> str:\n",
    "    \"\"\"List all saved note titles.\"\"\"\n",
    "    if not notes_db:\n",
    "        return \"No notes saved yet.\"\n",
    "    return \"Saved notes: \" + \", \".join(notes_db.keys())\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information. Returns simulated search results.\"\"\"\n",
    "    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê²€ìƒ‰ API ì—°ë™\n",
    "    return f\"[Simulated search results for '{query}']: Found relevant information about {query}. Key points: 1) Definition and overview, 2) Recent developments, 3) Practical applications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_V2 = \"\"\"You are a research assistant with the following capabilities:\n",
    "- Search the web for information\n",
    "- Save important findings as notes\n",
    "- Retrieve and list saved notes\n",
    "\n",
    "When researching a topic:\n",
    "1. Search for relevant information\n",
    "2. Summarize key findings\n",
    "3. Offer to save important points as notes\n",
    "\"\"\"\n",
    "\n",
    "agent_v2 = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[web_search, save_note, get_note, list_notes],\n",
    "    system_prompt=SYSTEM_PROMPT_V2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸: ê²€ìƒ‰í•˜ê³  ë©”ëª¨ ì €ì¥\n",
    "for step in agent_v2.stream(\n",
    "    {\"messages\": \"LLM Agentì— ëŒ€í•´ ê²€ìƒ‰í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ 'LLM_Agent_Basics'ë¼ëŠ” ì œëª©ìœ¼ë¡œ ì €ì¥í•´ì¤˜\"},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ë©”ëª¨ í™•ì¸\n",
    "result = agent_v2.invoke({\"messages\": \"ì €ì¥ëœ ë©”ëª¨ ëª©ë¡ ë³´ì—¬ì¤˜\"})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-problem",
   "metadata": {},
   "source": [
    "**ë¬¸ì œì :** ê° í˜¸ì¶œì´ ë…ë¦½ì ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: ë©”ëª¨ë¦¬ ì¶”ê°€\n",
    "\n",
    "ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ì—¬ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ì—ì´ì „íŠ¸ë¡œ ë°œì „ì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "agent_v3 = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[web_search, save_note, get_note, list_notes],\n",
    "    system_prompt=SYSTEM_PROMPT_V2,\n",
    "    checkpointer=InMemorySaver(),  # ë©”ëª¨ë¦¬ ì¶”ê°€\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread_idë¡œ ì„¸ì…˜ êµ¬ë¶„\n",
    "config = {\"configurable\": {\"thread_id\": \"session-1\"}}\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë©”ì‹œì§€\n",
    "result = agent_v3.invoke(\n",
    "    {\"messages\": \"ë‚´ ì´ë¦„ì€ ë¯¼ìˆ˜ì•¼. AI ì—ì´ì „íŠ¸ì— ëŒ€í•´ ì—°êµ¬í•˜ê³  ìˆì–´.\"},\n",
    "    config\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-test2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë²ˆì§¸ ë©”ì‹œì§€ - ì´ì „ ëŒ€í™” ê¸°ì–µí•˜ëŠ”ì§€ í™•ì¸\n",
    "result = agent_v3.invoke(\n",
    "    {\"messages\": \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€? ê·¸ë¦¬ê³  ë­˜ ì—°êµ¬í•œë‹¤ê³  í–ˆì–´?\"},\n",
    "    config  # ê°™ì€ thread_id ì‚¬ìš©\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-test3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ì„¸ì…˜ì€ ë…ë¦½ì \n",
    "config_new = {\"configurable\": {\"thread_id\": \"session-2\"}}\n",
    "\n",
    "result = agent_v3.invoke(\n",
    "    {\"messages\": \"ë‚´ ì´ë¦„ì´ ë­ì•¼?\"},\n",
    "    config_new\n",
    ")\n",
    "print(result[\"messages\"][-1].content)  # ëª¨ë¦„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-done",
   "metadata": {},
   "source": [
    "**ì„±ê³¼:** ì´ì œ ì—ì´ì „íŠ¸ê°€ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ê¸´ ì‘ë‹µë„ í† í° ë‹¨ìœ„ë¡œ ì¦‰ì‹œ í‘œì‹œë˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-stream-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_mode=\"values\": ê° ë‹¨ê³„ì˜ ì „ì²´ ìƒíƒœ\n",
    "config = {\"configurable\": {\"thread_id\": \"stream-test\"}}\n",
    "\n",
    "print(\"=== stream_mode='values' ===\")\n",
    "for step in agent_v3.stream(\n",
    "    {\"messages\": \"AI ì—ì´ì „íŠ¸ì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ì§§ê²Œ ë§í•´ì¤˜\"},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-stream-messages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_mode=\"messages\": í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "config = {\"configurable\": {\"thread_id\": \"stream-test-2\"}}\n",
    "\n",
    "print(\"=== stream_mode='messages' (í† í° ë‹¨ìœ„) ===\")\n",
    "for token, metadata in agent_v3.stream(\n",
    "    {\"messages\": \"AI ì—ì´ì „íŠ¸ì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ì§§ê²Œ ë§í•´ì¤˜\"},\n",
    "    config,\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    print(token.content, end=\"\", flush=True)\n",
    "print()  # ì¤„ë°”ê¿ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: êµ¬ì¡°í™”ëœ ì¶œë ¥\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì •í•´ì§„ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class ResearchReport(BaseModel):\n",
    "    \"\"\"ì—°êµ¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìŠ¤í‚¤ë§ˆ\"\"\"\n",
    "    topic: str\n",
    "    summary: str\n",
    "    key_points: List[str]\n",
    "    sources: List[str]\n",
    "    confidence_level: str  # \"high\", \"medium\", \"low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ì—ì´ì „íŠ¸\n",
    "agent_structured = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[web_search],\n",
    "    system_prompt=\"You are a research assistant. Search for information and compile a structured report.\",\n",
    "    response_format=ResearchReport,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_structured.invoke(\n",
    "    {\"messages\": \"LangChainì— ëŒ€í•´ ì¡°ì‚¬í•´ì¤˜\"}\n",
    ")\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì‘ë‹µ\n",
    "report = result[\"structured_response\"]\n",
    "print(f\"Topic: {report.topic}\")\n",
    "print(f\"Summary: {report.summary}\")\n",
    "print(f\"Key Points:\")\n",
    "for i, point in enumerate(report.key_points, 1):\n",
    "    print(f\"  {i}. {point}\")\n",
    "print(f\"Confidence: {report.confidence_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: ë™ì  í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "ëŸ°íƒ€ì„ ìƒí™©ì— ë”°ë¼ ì—ì´ì „íŠ¸ ë™ì‘ì„ ë³€ê²½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸\"\"\"\n",
    "    user_name: str\n",
    "    is_premium: bool\n",
    "    language: str  # \"ko\" or \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-dynamic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware.types import ModelRequest, dynamic_prompt\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are a research assistant for {user_name}.\n",
    "\n",
    "{tier_message}\n",
    "\n",
    "Language preference: Respond in {language}.\n",
    "\n",
    "Available tools: web_search, save_note, get_note, list_notes\n",
    "\"\"\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    ctx = request.runtime.context\n",
    "    \n",
    "    if ctx.is_premium:\n",
    "        tier_message = \"Premium user: Provide detailed, comprehensive responses with extra insights.\"\n",
    "    else:\n",
    "        tier_message = \"Free user: Provide concise responses. Limit to 3 search queries per topic.\"\n",
    "    \n",
    "    language = \"Korean\" if ctx.language == \"ko\" else \"English\"\n",
    "    \n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        user_name=ctx.user_name,\n",
    "        tier_message=tier_message,\n",
    "        language=language\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6 = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[web_search, save_note, get_note, list_notes],\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=UserContext,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-test-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ì\n",
    "premium_ctx = UserContext(user_name=\"ê¹€ì² ìˆ˜\", is_premium=True, language=\"ko\")\n",
    "\n",
    "result = agent_v6.invoke(\n",
    "    {\"messages\": \"AI ì—ì´ì „íŠ¸ì— ëŒ€í•´ ì•Œë ¤ì¤˜\"},\n",
    "    context=premium_ctx\n",
    ")\n",
    "print(\"=== Premium User Response ===\")\n",
    "print(result[\"messages\"][-1].content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6-test-free",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬´ë£Œ ì‚¬ìš©ì\n",
    "free_ctx = UserContext(user_name=\"John\", is_premium=False, language=\"en\")\n",
    "\n",
    "result = agent_v6.invoke(\n",
    "    {\"messages\": \"Tell me about AI agents\"},\n",
    "    context=free_ctx\n",
    ")\n",
    "print(\"=== Free User Response ===\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Human-in-the-Loop (HITL)\n",
    "\n",
    "ë¯¼ê°í•œ ì‘ì—… ì „ì— ì‚¬ìš©ì ìŠ¹ì¸ì„ ë°›ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¼ê°í•œ ì‘ì—…ì„ í¬í•¨í•œ ë„êµ¬\n",
    "@tool\n",
    "def delete_note(title: str) -> str:\n",
    "    \"\"\"Delete a saved note. This action cannot be undone.\"\"\"\n",
    "    if title in notes_db:\n",
    "        del notes_db[title]\n",
    "        return f\"Note '{title}' has been deleted.\"\n",
    "    return f\"Note '{title}' not found.\"\n",
    "\n",
    "@tool\n",
    "def export_all_notes() -> str:\n",
    "    \"\"\"Export all notes to external storage. Requires approval.\"\"\"\n",
    "    return f\"Exported {len(notes_db)} notes to external storage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "\n",
    "agent_v7 = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[web_search, save_note, get_note, list_notes, delete_note, export_all_notes],\n",
    "    system_prompt=\"You are a research assistant. Help users manage their research notes.\",\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            # deleteì™€ exportëŠ” ìŠ¹ì¸ í•„ìš”\n",
    "            interrupt_on={\n",
    "                \"delete_note\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
    "                \"export_all_notes\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë©”ëª¨ ì¶”ê°€\n",
    "notes_db[\"test_note\"] = {\"content\": \"This is a test note\", \"created_at\": \"2024-01-01\"}\n",
    "print(f\"Current notes: {list(notes_db.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"hitl-test\"}}\n",
    "\n",
    "# ì‚­ì œ ìš”ì²­\n",
    "result = agent_v7.invoke(\n",
    "    {\"messages\": \"test_note ë©”ëª¨ë¥¼ ì‚­ì œí•´ì¤˜\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# ì¸í„°ëŸ½íŠ¸ í™•ì¸\n",
    "if \"__interrupt__\" in result:\n",
    "    interrupt_info = result['__interrupt__'][-1].value['action_requests'][-1]\n",
    "    print(\"ğŸ›‘ ìŠ¹ì¸ í•„ìš”!\")\n",
    "    print(f\"   ë„êµ¬: {interrupt_info['tool_name']}\")\n",
    "    print(f\"   ì¸ì: {interrupt_info['tool_input']}\")\n",
    "    \n",
    "    # ì‚¬ìš©ì ìŠ¹ì¸ (ì‹¤ì œë¡œëŠ” UIì—ì„œ ì…ë ¥ë°›ìŒ)\n",
    "    user_decision = \"approve\"  # or \"reject\"\n",
    "    \n",
    "    if user_decision == \"approve\":\n",
    "        result = agent_v7.invoke(\n",
    "            Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
    "            config=config\n",
    "        )\n",
    "        print(\"\\nâœ… ìŠ¹ì¸ë¨\")\n",
    "    else:\n",
    "        result = agent_v7.invoke(\n",
    "            Command(resume={\"decisions\": [{\"type\": \"reject\", \"message\": \"User cancelled\"}]}),\n",
    "            config=config\n",
    "        )\n",
    "        print(\"\\nâŒ ê±°ë¶€ë¨\")\n",
    "\n",
    "print(f\"\\nê²°ê³¼: {result['messages'][-1].content}\")\n",
    "print(f\"í˜„ì¬ ë©”ëª¨: {list(notes_db.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ìµœì¢…: ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•œ ì™„ì„± ì—ì´ì „íŠ¸\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ì— í†µí•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì»¨í…ìŠ¤íŠ¸\"\"\"\n",
    "    user_id: str\n",
    "    user_name: str\n",
    "    is_premium: bool\n",
    "    language: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.config import get_stream_writer\n",
    "from datetime import datetime\n",
    "\n",
    "# ì‚¬ìš©ìë³„ ë©”ëª¨ ì €ì¥ì†Œ\n",
    "user_notes = {}\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information on a topic.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    writer(f\"ğŸ” Searching for: {query}\")\n",
    "    # ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ API í˜¸ì¶œ\n",
    "    writer(f\"âœ… Found results for: {query}\")\n",
    "    return f\"Search results for '{query}': [Relevant information found]\"\n",
    "\n",
    "@tool\n",
    "def save_research_note(title: str, content: str, tags: str) -> str:\n",
    "    \"\"\"Save a research note with title, content, and comma-separated tags.\"\"\"\n",
    "    from langgraph.runtime import get_runtime\n",
    "    runtime = get_runtime(AppContext)\n",
    "    user_id = runtime.context.user_id\n",
    "    \n",
    "    if user_id not in user_notes:\n",
    "        user_notes[user_id] = {}\n",
    "    \n",
    "    user_notes[user_id][title] = {\n",
    "        \"content\": content,\n",
    "        \"tags\": [t.strip() for t in tags.split(\",\")],\n",
    "        \"created_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    return f\"Note '{title}' saved with tags: {tags}\"\n",
    "\n",
    "@tool\n",
    "def get_my_notes() -> str:\n",
    "    \"\"\"Get all notes for the current user.\"\"\"\n",
    "    from langgraph.runtime import get_runtime\n",
    "    runtime = get_runtime(AppContext)\n",
    "    user_id = runtime.context.user_id\n",
    "    \n",
    "    if user_id not in user_notes or not user_notes[user_id]:\n",
    "        return \"No notes found.\"\n",
    "    \n",
    "    result = []\n",
    "    for title, note in user_notes[user_id].items():\n",
    "        result.append(f\"- {title} (tags: {', '.join(note['tags'])})\")\n",
    "    return \"Your notes:\\n\" + \"\\n\".join(result)\n",
    "\n",
    "@tool\n",
    "def delete_research_note(title: str) -> str:\n",
    "    \"\"\"Delete a research note. Requires user approval.\"\"\"\n",
    "    from langgraph.runtime import get_runtime\n",
    "    runtime = get_runtime(AppContext)\n",
    "    user_id = runtime.context.user_id\n",
    "    \n",
    "    if user_id in user_notes and title in user_notes[user_id]:\n",
    "        del user_notes[user_id][title]\n",
    "        return f\"Note '{title}' deleted.\"\n",
    "    return f\"Note '{title}' not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-dynamic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware.types import ModelRequest, dynamic_prompt\n",
    "\n",
    "FINAL_PROMPT = \"\"\"You are a research assistant for {user_name}.\n",
    "\n",
    "User tier: {tier}\n",
    "{tier_instructions}\n",
    "\n",
    "Language: Always respond in {language}.\n",
    "\n",
    "Capabilities:\n",
    "- search_web: Search for information\n",
    "- save_research_note: Save notes with tags\n",
    "- get_my_notes: List user's saved notes\n",
    "- delete_research_note: Delete a note (requires approval)\n",
    "\n",
    "Be helpful, concise, and organize information clearly.\n",
    "\"\"\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def build_prompt(request: ModelRequest) -> str:\n",
    "    ctx = request.runtime.context\n",
    "    \n",
    "    tier = \"Premium\" if ctx.is_premium else \"Free\"\n",
    "    tier_instructions = (\n",
    "        \"Provide comprehensive, detailed responses with insights.\"\n",
    "        if ctx.is_premium else\n",
    "        \"Provide concise responses. Suggest premium for advanced features.\"\n",
    "    )\n",
    "    language = \"Korean\" if ctx.language == \"ko\" else \"English\"\n",
    "    \n",
    "    return FINAL_PROMPT.format(\n",
    "        user_name=ctx.user_name,\n",
    "        tier=tier,\n",
    "        tier_instructions=tier_instructions,\n",
    "        language=language\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# ìµœì¢… ì—ì´ì „íŠ¸: ëª¨ë“  ê¸°ëŠ¥ í†µí•©\n",
    "final_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[search_web, save_research_note, get_my_notes, delete_research_note],\n",
    "    middleware=[\n",
    "        build_prompt,\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"delete_research_note\": {\"allowed_decisions\": [\"approve\", \"reject\"]}\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    context_schema=AppContext,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"âœ… Final Research Assistant Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ëª¨: ì™„ì„±ëœ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "user_ctx = AppContext(\n",
    "    user_id=\"user-001\",\n",
    "    user_name=\"ë¯¼ìˆ˜\",\n",
    "    is_premium=True,\n",
    "    language=\"ko\"\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"final-demo\"}}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¤– Research Assistant Demo\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-demo-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ê²€ìƒ‰ + ìŠ¤íŠ¸ë¦¬ë°\n",
    "print(\"\\n[1] ê²€ìƒ‰ ë° ë©”ëª¨ ì €ì¥\")\n",
    "for chunk in final_agent.stream(\n",
    "    {\"messages\": \"LangChainì— ëŒ€í•´ ê²€ìƒ‰í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ë©”ëª¨ë¡œ ì €ì¥í•´ì¤˜. íƒœê·¸ëŠ” 'AI, Framework, LLM'ìœ¼ë¡œ í•´ì¤˜\"},\n",
    "    config,\n",
    "    context=user_ctx,\n",
    "    stream_mode=[\"values\", \"custom\"]\n",
    "):\n",
    "    if chunk[0] == \"custom\":\n",
    "        print(f\"  {chunk[1]}\")\n",
    "    elif chunk[0] == \"values\":\n",
    "        msg = chunk[1][\"messages\"][-1]\n",
    "        if hasattr(msg, 'content') and msg.content:\n",
    "            pass  # ìµœì¢… ì‘ë‹µì€ ë§ˆì§€ë§‰ì— ì¶œë ¥\n",
    "\n",
    "# ìµœì¢… ì‘ë‹µ\n",
    "result = final_agent.invoke({\"messages\": \"ë°©ê¸ˆ ë­˜ í–ˆì–´?\"}, config, context=user_ctx)\n",
    "print(f\"\\nì‘ë‹µ: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-demo-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ë©”ëª¨ë¦¬ í™•ì¸ (ì´ì „ ëŒ€í™” ê¸°ì–µ)\n",
    "print(\"\\n[2] ë©”ëª¨ë¦¬ í™•ì¸\")\n",
    "result = final_agent.invoke(\n",
    "    {\"messages\": \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€? ì €ì¥ëœ ë©”ëª¨ë„ ë³´ì—¬ì¤˜\"},\n",
    "    config,\n",
    "    context=user_ctx\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ìš”ì•½: ì‹±ê¸€ ì—ì´ì „íŠ¸ êµ¬ì¶• ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "| ë‹¨ê³„ | êµ¬ì„±ìš”ì†Œ | ì½”ë“œ |  |\n",
    "|------|----------|------|---|\n",
    "| 1 | ê¸°ë³¸ ì—ì´ì „íŠ¸ | `create_agent(model, system_prompt)` | L1 |\n",
    "| 2 | ë„êµ¬ | `@tool` decorator + `tools=[...]` | L4 |\n",
    "| 3 | ë©”ëª¨ë¦¬ | `checkpointer=InMemorySaver()` | L6 |\n",
    "| 4 | ìŠ¤íŠ¸ë¦¬ë° | `stream_mode=\"messages\"` or `\"values\"` | L3 |\n",
    "| 5 | êµ¬ì¡°í™” ì¶œë ¥ | `response_format=YourSchema` | L7 |\n",
    "| 6 | ë™ì  í”„ë¡¬í”„íŠ¸ | `@dynamic_prompt` + `middleware=[...]` | L8 |\n",
    "| 7 | HITL | `HumanInTheLoopMiddleware` | L9 |\n",
    "\n",
    "### í•µì‹¬ íŒ¨í„´\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",           # LLM\n",
    "    tools=[tool1, tool2],                  # ë„êµ¬ë“¤\n",
    "    system_prompt=\"...\",                   # ë˜ëŠ” middlewareë¡œ ë™ì  í”„ë¡¬í”„íŠ¸\n",
    "    middleware=[dynamic_prompt, hitl],     # ë¯¸ë“¤ì›¨ì–´\n",
    "    context_schema=MyContext,              # ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸\n",
    "    checkpointer=InMemorySaver(),          # ë©”ëª¨ë¦¬\n",
    "    response_format=MySchema,              # êµ¬ì¡°í™” ì¶œë ¥ (ì„ íƒ)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì´ ì—ì´ì „íŠ¸ë¥¼ ë” ë°œì „ì‹œí‚¤ë ¤ë©´:\n",
    "\n",
    "1. **ì‹¤ì œ ê²€ìƒ‰ API ì—°ë™**: Tavily, Serper, Google Search API\n",
    "2. **ì˜êµ¬ ì €ì¥ì†Œ**: PostgreSQL, Redisë¡œ checkpointer êµì²´\n",
    "3. **MCP ë„êµ¬ ì¶”ê°€**: `langchain-mcp-adapters`ë¡œ ì™¸ë¶€ MCP ì„œë²„ ì—°ê²° (L5)\n",
    "4. **ë©€í‹° ì—ì´ì „íŠ¸**: ì—¬ëŸ¬ ì—ì´ì „íŠ¸ í˜‘ì—… (ë‹¤ìŒ ì½”ìŠ¤)\n",
    "5. **í”„ë¡œë•ì…˜ ë°°í¬**: LangGraph Cloud, API ì„œë²„í™”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-to-explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
