{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190a3c14-4a67-4d64-9378-0b9737e4a5f7",
   "metadata": {},
   "source": [
    "# âœ‰ï¸ Messages\n",
    "# âœ‰ï¸ ë©”ì‹œì§€\n",
    "  <img src=\"./assets/LC_Messages.png\" width=\"500\">\n",
    "\n",
    "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n",
    "\n",
    "ë©”ì‹œì§€ëŠ” LangChainì—ì„œ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤. ë©”ì‹œì§€ëŠ” ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë‚˜íƒ€ë‚´ë©°, LLMê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ëŒ€í™” ìƒíƒœë¥¼ í‘œí˜„í•˜ëŠ” ë° í•„ìš”í•œ ì½˜í…ì¸ ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ ë‹´ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c566d2-7844-4901-af65-4a6e58817716",
   "metadata": {},
   "source": [
    "## Setup\n",
    "## ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6ad0d-4efd-4066-9a8d-ca8bb0de94ef",
   "metadata": {},
   "source": [
    "Load and/or check for needed environmental variables\n",
    "\n",
    "í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ í™•ì¸í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa9fb12-98e3-490d-9ae6-885054c8f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY=<not set>\n",
      "LANGSMITH_API_KEY=<not set>\n",
      "LANGSMITH_TRACING=<not set>\n",
      "LANGSMITH_PROJECT=<not set>\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print results\n",
    "doublecheck_env(\"example.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f27b5-a53a-4f24-9480-6af61823d4e6",
   "metadata": {},
   "source": [
    "## HumanğŸ‘¨â€ğŸ’» and AI ğŸ¤– Messages\n",
    "## HumanğŸ‘¨â€ğŸ’» ë©”ì‹œì§€ì™€ AI ğŸ¤– ë©”ì‹œì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca38b8-7514-4e18-b141-86f77c684ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m agent = \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai:gpt-5-nano\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a full-stack comedian\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/langchain/agents/factory.py:607\u001b[39m, in \u001b[36mcreate_agent\u001b[39m\u001b[34m(model, tools, system_prompt, middleware, response_format, state_schema, context_schema, checkpointer, store, interrupt_before, interrupt_after, debug, name, cache)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# init chat model\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     model = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# Handle tools being None or empty\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:284\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     warnings.warn(\n\u001b[32m    277\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    280\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    281\u001b[39m     )\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    290\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:311\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_openai\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_provider == \u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    313\u001b[39m     _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_anthropic\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:113\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:914\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    904\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_async_client = httpx.AsyncClient(\n\u001b[32m    905\u001b[39m             proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy, verify=global_ssl_context\n\u001b[32m    906\u001b[39m         )\n\u001b[32m    907\u001b[39m     async_specific = {\n\u001b[32m    908\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_async_client\n\u001b[32m    909\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: async_api_key_value,\n\u001b[32m    913\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_async_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28mself\u001b[39m.async_client = \u001b[38;5;28mself\u001b[39m.root_async_client.chat.completions\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/agent_projects/lca-langchainV1-essentials/python/.venv/lib/python3.13/site-packages/openai/_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    486\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\", \n",
    "    system_prompt=\"You are a full-stack comedian\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e517775-cdac-43ab-a40b-1a9e8deaa666",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_msg = HumanMessage(\"Hello, how are you?\")\n",
    "\n",
    "result = agent.invoke({\"messages\": [human_msg]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bcefc1-53d7-4723-a3dc-d87983be761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! Iâ€™m doing wellâ€”stack is healthy, coffee levels are optimal, and Iâ€™ve got a fresh batch of jokes ready. How are you?\n",
      "\n",
      "A couple quick techy one-liners to start:\n",
      "- Why do programmers prefer dark mode? Because light attracts bugs.\n",
      "- I would tell you a UDP joke, but you might not get it.\n",
      "- My code runs on coffee and sarcasmâ€”mostly sarcasm.\n",
      "\n",
      "What can I help you with today? Want a joke, a debugging tip, a quick code snippet, or a plan for a stand-up bit about full-stack life?\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c37ca3-70f7-4f76-8108-94210ba7f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result[\"messages\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e186f7e-8818-4de4-bebd-4f73cebe5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Hello, how are you?\n",
      "\n",
      "ai: Hey there! Iâ€™m doing wellâ€”stack is healthy, coffee levels are optimal, and Iâ€™ve got a fresh batch of jokes ready. How are you?\n",
      "\n",
      "A couple quick techy one-liners to start:\n",
      "- Why do programmers prefer dark mode? Because light attracts bugs.\n",
      "- I would tell you a UDP joke, but you might not get it.\n",
      "- My code runs on coffee and sarcasmâ€”mostly sarcasm.\n",
      "\n",
      "What can I help you with today? Want a joke, a debugging tip, a quick code snippet, or a plan for a stand-up bit about full-stack life?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for msg in result[\"messages\"]:\n",
    "    print(f\"{msg.type}: {msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f30337-3873-4b0d-aeff-3c418f5dff32",
   "metadata": {},
   "source": [
    "### Altenative formats\n",
    "### ëŒ€ì²´ í˜•ì‹\n",
    "#### Strings\n",
    "#### ë¬¸ìì—´\n",
    "There are situations where LangChain can infer the role from the context, and a simple string is enough to create a message.\n",
    "\n",
    "LangChainì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì—­í• ì„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ì´ë•ŒëŠ” ê°„ë‹¨í•œ ë¬¸ìì—´ë§Œìœ¼ë¡œë„ ë©”ì‹œì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2450c1b-924a-422c-bac3-62b1f03dc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    system_prompt=\"You are a terse sports poet.\",  # This is a SystemMessage under the hood\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9a9f83-79d4-4abe-b1e3-45ef58d2f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball: chalked diamond, sun on the infield.\n",
      "The pitcher winds; the ball answers with heat.\n",
      "The bat cracks; silence shatters, the crowd breathes.\n",
      "Leather speaks; gloves swallow ground.\n",
      "Three strikes, the inning exhales and dies.\n",
      "Four ballsâ€”walk to first, a quiet advance.\n",
      "A stolen baseâ€”a blur of grass and steel.\n",
      "Nine innings, time stitched in dirt and hope.\n",
      "Rain delays, legends, late-inning miracles.\n",
      "Seventh-inning stretch; hats rise to the anthem.\n",
      "\n",
      "Baseball endures, old dream, fresh memory.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": \"Tell me about baseball\"})   # This is a HumanMessage under the hood\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1544d7-3590-42e9-a56b-0dfb34f28505",
   "metadata": {},
   "source": [
    "#### Dictionaries\n",
    "#### ë”•ì…”ë„ˆë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b11e46-9c25-4828-9c6a-29a4594acdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sprinters surge forward\n",
      "Sun sears the track as feet scream\n",
      "Finish lines shiver\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": {\"role\": \"user\", \"content\": \"Write a haiku about sprinters\"}}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00905cbe-b248-496f-898c-d223cd1fd0d7",
   "metadata": {},
   "source": [
    "There are multiple roles:\n",
    "\n",
    "ì—¬ëŸ¬ ì—­í• ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sports poetry expert who completes haikus that have been started\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about sprinters\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Feet don't fail me...\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b31b4c-00f0-4b37-8152-6f243590df8e",
   "metadata": {},
   "source": [
    "## Output Format\n",
    "## ì¶œë ¥ í˜•ì‹\n",
    "### messages\n",
    "### ë©”ì‹œì§€\n",
    "Let's create a tool so agent will create some tool messages.\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ë„êµ¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ë„ë¡ ë„êµ¬ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27831c76-be27-4ee8-a24d-cc6d455f4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def check_haiku_lines(text: str):\n",
    "    \"\"\"Check if the given haiku text has exactly 3 lines.\n",
    "\n",
    "    Returns None if it's correct, otherwise an error message.\n",
    "    \"\"\"\n",
    "    # Split the text into lines, ignoring leading/trailing spaces\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    print(f\"checking haiku, it has {len(lines)} lines:\\n {text}\")\n",
    "\n",
    "    if len(lines) != 3:\n",
    "        return f\"Incorrect! This haiku has {len(lines)} lines. A haiku must have exactly 3 lines.\"\n",
    "    return \"Correct, this haiku has 3 lines.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879cad42-e41c-4d03-a118-585ff9dcfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5\",\n",
    "    tools=[check_haiku_lines],\n",
    "    system_prompt=\"You are a sports poet who only writes Haiku. You always check your work.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a70fbb-1d26-411c-aa87-6077bdf21868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking haiku, it has 3 lines:\n",
      " Whistle splits cool air\n",
      "Stadium hearts drum in time\n",
      "Footsteps chase the dawn\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": \"Please write me a poem\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b3f31f-7247-4d9c-811d-48b041d8eb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whistle splits cool air\\nStadium hearts drum in time\\nFootsteps chase the dawn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605faf3d-c76f-499d-abde-6fe0473eea52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91779142-2d3a-4e9e-9827-69c538e8f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Please write me a poem\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  check_haiku_lines (call_uOLf0ZxcBTNBT2a77IZJi1jO)\n",
      " Call ID: call_uOLf0ZxcBTNBT2a77IZJi1jO\n",
      "  Args:\n",
      "    text: Whistle splits cool air\n",
      "Stadium hearts drum in time\n",
      "Footsteps chase the dawn\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: check_haiku_lines\n",
      "\n",
      "Correct, this haiku has 3 lines.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Whistle splits cool air\n",
      "Stadium hearts drum in time\n",
      "Footsteps chase the dawn\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c704dd-baf5-4afd-a89c-ef3790fe1310",
   "metadata": {},
   "source": [
    "### Other useful information\n",
    "### ê¸°íƒ€ ìœ ìš©í•œ ì •ë³´\n",
    "Above, the print messages have just been selecting pieces of the information stored in the messages list. Let's dig into all the information that is available!\n",
    "\n",
    "ìœ„ì—ì„œ ì¶œë ¥ëœ ë©”ì‹œì§€ë“¤ì€ messages ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ ì •ë³´ì˜ ì¼ë¶€ë§Œ ì„ íƒí•œ ê²ƒì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ ìì„¸íˆ ì‚´í´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1afcfa8-a706-403f-8c29-1f441d174908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Please write me a poem', additional_kwargs={}, response_metadata={}, id='13b18ae4-f92e-4582-925a-39d7984806a8'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 555, 'prompt_tokens': 170, 'total_tokens': 725, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CQbI0V3W0uPCDlcnD4GYTydYxCsAJ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0055426a-ee06-4daa-a36c-4074fa9c1ceb-0', tool_calls=[{'name': 'check_haiku_lines', 'args': {'text': 'Whistle splits cool air\\nStadium hearts drum in time\\nFootsteps chase the dawn'}, 'id': 'call_uOLf0ZxcBTNBT2a77IZJi1jO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 555, 'total_tokens': 725, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}}),\n",
       "  ToolMessage(content='Correct, this haiku has 3 lines.', name='check_haiku_lines', id='5aff8da9-c8c0-4bed-b8d4-7bf884a8c097', tool_call_id='call_uOLf0ZxcBTNBT2a77IZJi1jO'),\n",
       "  AIMessage(content='Whistle splits cool air\\nStadium hearts drum in time\\nFootsteps chase the dawn', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 231, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CQbIATueaDYKiqYDNX5xKHjKGWCr5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--61eb6a33-1bda-46d7-8d00-f3aaea2487a3-0', usage_metadata={'input_tokens': 231, 'output_tokens': 21, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b0568f-41d7-48e3-b69e-f2b8123d941a",
   "metadata": {},
   "source": [
    "You can select just the last message, and you can see where the final message is coming from.\n",
    "\n",
    "ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì„ íƒí•  ìˆ˜ ìˆìœ¼ë©°, ìµœì¢… ë©”ì‹œì§€ê°€ ì–´ë””ì„œ ì™”ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bacc660-7997-4da7-9d3e-eee4b8119601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Whistle splits cool air\\nStadium hearts drum in time\\nFootsteps chase the dawn', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 231, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CQbIATueaDYKiqYDNX5xKHjKGWCr5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--61eb6a33-1bda-46d7-8d00-f3aaea2487a3-0', usage_metadata={'input_tokens': 231, 'output_tokens': 21, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7254b9e5-f6ac-432e-bf9a-05676f8e4b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 231,\n",
       " 'output_tokens': 21,\n",
       " 'total_tokens': 252,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1].usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "523f453e-a425-4df0-88b0-a04e6e7612ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 21,\n",
       "  'prompt_tokens': 231,\n",
       "  'total_tokens': 252,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_provider': 'openai',\n",
       " 'model_name': 'gpt-5-2025-08-07',\n",
       " 'system_fingerprint': None,\n",
       " 'id': 'chatcmpl-CQbIATueaDYKiqYDNX5xKHjKGWCr5',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-1].response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5d505-a2db-4f64-9346-03af057b3be6",
   "metadata": {},
   "source": [
    "### Try it on your own!\n",
    "### ì§ì ‘ í•´ë³´ì„¸ìš”!\n",
    "Change the system prompt, use the `pretty_printer` to print some messages or dig through `results` on your own. Notice the Human, AI and Tool messages and some of their associated metadata. Notice how the final results provide a complete history of the agents activity!\n",
    "\n",
    "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ê²½í•˜ê³ , `pretty_printer`ë¥¼ ì‚¬ìš©í•´ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê±°ë‚˜ `results`ë¥¼ ì§ì ‘ íƒìƒ‰í•´ ë³´ì„¸ìš”. Human, AI, Tool ë©”ì‹œì§€ì™€ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”. ìµœì¢… ê²°ê³¼ê°€ ì—ì´ì „íŠ¸ í™œë™ì˜ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ì–´ë–»ê²Œ ì œê³µí•˜ëŠ”ì§€ ì£¼ëª©í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f921687f-005c-4727-b041-18bafbfaf1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5\",\n",
    "    tools=[check_haiku_lines],\n",
    "    system_prompt=\"Your SYSTEM prompt here\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e27be2-6bf9-4c0e-b466-8f9e483bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-to-explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
